#! /usr/bin/env python
#
# The Parker Lab
# theparkerlab.org
#
# University of Michigan, Ann Arbor
#

import os
import sys
import shutil
import itertools
import subprocess
from functools import partial


if not config.get("results"):
    print("`results` dir not set. Check config. Exiting.")
    sys.exit(1)

if not config.get("email"):
    print("WARN: User email not set. No notifications will be sent!")

if not config.get("params"):
    print("WARN: Parameters are not set, will use default where possible")
    config["params"] = {}


##
## Metadata
##
ORGANISMS = {
    "rn4": "rat",
    "rn5": "rat",
    "rn6": "rat",
    "mm9": "mouse",
    "mm10": "mouse",
    "hg19": "human",
    "hg38": "human",
}

AUTOSOMAL_REFERENCES = {
    "hg19": [f"chr{i}" for i in range(1, 23)],
    "hg38": [f"chr{i}" for i in range(1, 23)],
    "mm9": [f"chr{i}" for i in range(1, 20)],
    "mm10": [f"chr{i}" for i in range(1, 20)],
    "rn4": [f"chr{i}" for i in range(1, 21)],
    "rn5": [f"chr{i}" for i in range(1, 21)],
    "rn6": [f"chr{i}" for i in range(1, 21)],
}

MACS2_GENOME_SIZE = {
    "rn4": "mm",
    "rn5": "mm",
    "rn6": "mm",
    "mm9": "mm",
    "mm10": "mm",
    "hg19": "hs",
    "hg38": "hs",
}


def check_utils_version(outfile):
    """Check if the tools exists, and if they do, collect their version number.
    Otherwise, exit with an unpleasant error message."""

    utils = ["fastqc -v", "cta --version", "cutadapt --version",
             "samtools --version", "bwa", "macs2 --version", "ataqv --version",
             "picard --version", "bedtools --version", "pigz --version"]

    f_out, STATUS_CODE = open(outfile, "w"), True

    for util in utils:
        if not shutil.which(util.split()[0]):
            msg = f"{util} not found!\n"
            STATUS_CODE = False
        else:
            # should not fail with a "CalledProcessError" since we've already
            # checked if the binary exists or not.
            msg = subprocess.check_output(util, shell=True, encoding="utf8")

        f_out.write(msg)

    f_out.close()
    return STATUS_CODE


##
## Generate paths
##
_results = partial(os.path.join, config["results"])
_libraries = partial(_results, "libraries")
_samples = partial(_results, "samples")
_downsampled = partial(_results, "downsampled")
_log = partial(_results, "log")
_versions = partial(_results, "versions")


##
## Helper functions
##
def iterate_all_samples():
    return set([library["sample"] for library in config["libraries"].values()])


def iterate_all_libraries():
    return sorted(config["libraries"].keys())


def iterate_library_readgroups(library):
    return sorted(config["libraries"][library]["readgroups"].keys())


def iterate_sample_libraries(sample):
    for k, v in config["libraries"].items():
        if v["sample"] == sample:
            yield k


def library_to_sample(library):
    return config["libraries"][library]["sample"]


def readgroup_to_library(readgroup):
    for library in iterate_all_libraries():
        for library_readgroup in iterate_library_readgroups(library):
            if readgroup == library_readgroup:
                return library


def iterate_all_readgroups():
    for library in iterate_all_libraries():
        for readgroup in iterate_library_readgroups(library):
            yield readgroup


def list_readgroup_fastqs(readgroup):
    library = readgroup_to_library(readgroup)
    return config["libraries"][library]["readgroups"][readgroup]


def iterate_all_fastqs():
    for readgroup in iterate_all_readgroups():
        for fastq in list_readgroup_fastqs(readgroup):
            yield fastq


def fastq_basename_to_fastq(fastq_basename):
    for fastq in iterate_all_fastqs():
        if fastq_basename == os.path.basename(fastq):
            return fastq
    print(
        f"FATAL: Could not find FASTQ file for {fastq_basename}; Exiting.",
        file=sys.stderr
    )
    sys.exit(1)


def fastq_to_clipped_fastq(path, fastq):
    clipped_fastq_basename = os.path.basename(fastq).replace(
        ".fastq.gz", ".cl.fastq.gz"
    )
    return os.path.join(path, clipped_fastq_basename)


def get_library_genome(library):
    return config['libraries'][library]['genome']


def get_sample_genome(sample):
    for lib in iterate_all_libraries():
        if config['libraries'][lib]['sample'] == sample:
            return config['libraries'][lib]['genome']


def get_organism(genome):
    return ORGANISMS[genome]


def get_autosomes(genome):
    return AUTOSOMAL_REFERENCES[genome]


def get_bwa_index(genome):
    return config["bwa_index"][genome]


def get_tss(genome):
    return config["tss"][genome]


def get_chrom_sizes(genome):
    return config["chrom_sizes"][genome]


def get_whitelists(genome):
    return config["whitelist"].get(genome)


def get_blacklists(genome):
    return config["blacklist"].get(genome)


def strip_ext(fastq, ext=".fastq.gz"):
    return os.path.basename(fastq).replace(ext, "")


##
## Pipeline
##
##   `rule all` is at the end.
##

rule make_libraries:
    input:
        expand(
            _libraries("fastqc", "{fastq}_fastqc.zip"),
            fastq=[strip_ext(x) for x in iterate_all_fastqs()]
        ),
        expand(
            _libraries("fastqc_post-clip", "{fastq}.cl_fastqc.zip"),
            fastq=[strip_ext(x) for x in iterate_all_fastqs()]
        ),
        expand(
            _libraries("ataqv", "{library}.ataqv.json.gz"),
            library=iterate_all_libraries()
        ),
        _libraries("macs2", "all-libraries.master-peaks.bed"),


rule raw_fastqc:
    input:
        lambda wildcards: fastq_basename_to_fastq(
            f"{wildcards.fastq_basename}.fastq.gz"
        )
    output:
        _libraries("fastqc", "{fastq_basename}_fastqc.zip")
    params:
        outdir = _libraries("fastqc")
    log:
        _log("fastqc.{fastq_basename}.log")
    shell:
        """fastqc {input} -o {params.outdir} &> {log}"""


rule trim:
    """Use cta to trim adapter sequences.

    See https://github.com/ParkerLab/cta.
    """
    input:
        R1 = lambda wildcards: fastq_basename_to_fastq(
            f"{wildcards.fastq_basename}.R1.fastq.gz"
        ),
        R2 = lambda wildcards: fastq_basename_to_fastq(
            f"{wildcards.fastq_basename}.R2.fastq.gz"
        )
    output:
        R1 = _libraries("trimmed", "{fastq_basename}.R1.tr.fastq.gz"),
        R2 = _libraries("trimmed", "{fastq_basename}.R2.tr.fastq.gz")
    shell:
        """cta {input.R1} {input.R2} {output.R1} {output.R2}"""


rule clip_reads:
    """Optionally, clip the reads for a given list of samples"""
    input:
        R1 = _libraries("trimmed", "{fastq_basename}.R1.tr.fastq.gz"),
        R2 = _libraries("trimmed", "{fastq_basename}.R2.tr.fastq.gz")
    output:
        R1 = _libraries("clipped", "{fastq_basename}.R1.cl.fastq.gz"),
        R2 = _libraries("clipped", "{fastq_basename}.R2.cl.fastq.gz")
    params:
        libraries = config["params"].get("clip_samples"),
        length = config["params"].get("clip_length")
    run:
        #
        ## If FASTQ needs clipping, use cutadapt otherwise simply symlink
        #
        if wildcards.fastq_basename in params.libraries:
            shell(
                "cutadapt -u {params.length} -U {params.length} \
                -o {output.R1} -p {output.R2} {input.R1} {input.R2}"
            )
        else:
            shell(
                "ln -s {input.R1} {output.R1} \
                    && ln -s {input.R2} {output.R2}"
            )


rule post_fastqc:
    """Run FastQC after trimming and clipping"""
    input:
        _libraries("clipped", "{fastq_basename}.cl.fastq.gz"),
    output:
        _libraries("fastqc_post-clip", "{fastq_basename}.cl_fastqc.zip")
    params:
        outdir = _libraries("fastqc_post-clip")
    log:
        _log("fastqc.{fastq_basename}.log")
    shell:
        """
        fastqc {input} -o {params.outdir} &> {log}
        """


rule map:
    """Map reads to reference using BWA MEM algorithm.

    For ATAC-seq data, we use `-I 200,200,5000` flag which correspond to the
    mean, standard deviation, and max limits of the insert size distribution.
    Reads outside this are ignored by BWA. The defaults inferred by BWA are
    not best suited for the typical ATAC-seq insert size distribution if you
    analysis uses larger size fragments.
    """
    input:
        R1 = lambda wildcards: fastq_to_clipped_fastq(
            _libraries("clipped"),
            list_readgroup_fastqs(wildcards.readgroup)[0]
        ),
        R2 = lambda wildcards: fastq_to_clipped_fastq(
            _libraries("clipped"),
            list_readgroup_fastqs(wildcards.readgroup)[1]
        ),
        index = lambda wildcards: get_bwa_index(get_library_genome(wildcards.library))
    output:
        _libraries("bwa", "{library}______{readgroup}.bam")
    params:
        rg = "\\t".join(["@RG", "ID:{readgroup}", "LB:{library}"])
    threads: 4
    log:
        bwa = _log("map.bwa.{library}______{readgroup}.log"),
        samtools = _log("map.samtools.{library}______{readgroup}.log")
    shell:
        """
        bwa mem             \
            -M              \
            -R              \
            \"{params.rg}\" \
            -I 200,200,5000 \
            -t {threads}    \
            {input.index}   \
            {input.R1}   \
            {input.R2}  \
            2> {log.bwa}    \
        | samtools sort -m 2G -@ {threads} -O bam -o {output} - 2> {log.samtools}
        """


rule merge_readgroups:
    """Readgroups for a library are merged together into a single BAM file."""
    input:
        lambda wildcards: [
            _libraries("bwa", f"{wildcards.library}______{readgroup}.bam")
            for readgroup in iterate_library_readgroups(wildcards.library)
        ]
    output:
        _libraries("merge_readgroups", "{library}.bam")
    resources: io_concurrent = 1
    threads: 2
    shell:
        """samtools merge -@{threads} {output} {input}"""


rule mark_duplicates:
    """Use picard tools to mark duplicates for each library. Readgroups are
    merged in previous step as Picard can honor Readgroup tag and remove
    duplicates in a read-group aware manner."""
    input:
        bam = _libraries("merge_readgroups", "{library}.bam")
    output:
        bam = _libraries("mark_duplicates", "{library}.md.bam"),
        bai = _libraries("mark_duplicates", "{library}.md.bam.bai")
    params:
        metrics = _libraries("mark_duplicates", "{library}.md.metrics"),
        tmp_dir = config["params"].get("tmp_dir")
    resources: io_concurrent = 2
    log:
        _log("{library}.md.log")
    shell:
        # TODO: Don"t use picard wrapper; use native picard.jar invokation
        """
        picard -m 8G MarkDuplicates       \
            I={input.bam}                 \
            O={output.bam}                \
            ASSUME_SORTED=true            \
            METRICS_FILE={params.metrics} \
            VALIDATION_STRINGENCY=LENIENT \
            TMP_DIR={params.tmp_dir} &> {log}

        samtools index {output.bam}
        """


rule prune:
    """Prune all non-autosomal reads, marked duplicates, and unpaired reads
    giving only properly paired and aligned reads above specified alignment
    quality threshold.
    """
    input:
        bam = _libraries("mark_duplicates", "{library}.md.bam"),
        bai = _libraries("mark_duplicates", "{library}.md.bam.bai")
    output:
        bam = _libraries("pruned", "{library}.pd.bam"),
        bai = _libraries("pruned", "{library}.pd.bam.bai")
    params:
        tmp_dir = _libraries("pruned"),
        mapq = config.get("params_prune_mapq", 30),
        autosomes = lambda wildcards: get_autosomes(get_library_genome(wildcards.library))
    shell:
        """
        samtools view -b -h -f 3 -F 4 -F 8 -F 256 -F 1024 -F 2048 \
            -q {params.mapq} {input.bam} {params.autosomes} > {output.bam}

        samtools index {output.bam}
        """


rule bam2bed:
    input:
        _libraries("pruned", "{library}.pd.bam")
    output:
        _libraries("bam2bed", "{library}.pd.bed")
    shell:
        """bedtools bamtobed -i {input} > {output}"""


rule peaks:
    input:
        _libraries("bam2bed", "{library}.pd.bed")
    output:
        peaks = _libraries("macs2", "{library}_peaks.broadPeak"),
        bdg = _libraries("macs2", "{library}_treat_pileup.bdg.gz"),
    params:
        name = "{library}",
        outdir = _libraries("macs2"),
        genome_size = lambda wildcards: MACS2_GENOME_SIZE[get_library_genome(wildcards.library)]
    log:
        _log("{library}.macs2.out")
    shell:
        """
        macs2 callpeak \
          --outdir {params.outdir} \
          -t {input} \
          -n {params.name} \
          -f BED \
          -g {params.genome_size} \
          --nomodel \
          --shift -100 \
          --extsize 200 \
          --seed 2018 \
          -B \
          --broad \
          --keep-dup all \
          --SPMR \
          &> {log}

        pigz {params.outdir}/{wildcards.library}_treat_pileup.bdg
        """


rule blacklist_filter:
    input:
        _libraries("macs2", "{library}_peaks.broadPeak")
    output:
        _libraries("macs2", "{library}_peaks.broadPeak.noblacklist")
    params:
        blacklists = lambda wildcards: " ".join(
            get_blacklists(get_library_genome(wildcards.library))
        )
    shell:
        """mappabilityFilter -i {input} -b {params.blacklists} > {output}"""


rule merged_master_peaks:
    input:
        lambda wildcards: [
            _libraries("macs2", f"{library}_peaks.broadPeak.noblacklist")
            for library in iterate_all_libraries()
        ]
    output:
        _libraries("macs2", "all-libraries.master-peaks.bed")
    params:
        fdr = config.get("params_macs2_fdr", 0.05)
    shell:
        """createMasterPeaks {input} --fdr {params.fdr} > {output}"""


rule ataqv:
    """Ataqv-toolkit is a ATAC-seq experiment QC tool developed in Parker Lab.
    The tools provides many useful metrics such as Fragment Length
    Distribution, TSS Enrichment, for quality comparison and downstream
    analysis.

    See: https://github.com/ParkerLab/ataqv
    """
    input:
        bam = _libraries("mark_duplicates", "{library}.md.bam"),
        peaks = _libraries("macs2", "{library}_peaks.broadPeak.noblacklist")
    output:
        _libraries("ataqv", "{library}.ataqv.json.gz")
    params:
        name = "{library}",
        description = "{library}",
        organism = lambda wildcards: get_organism(get_library_genome(wildcards.library)),
        tss = lambda wildcards: get_tss(get_library_genome(wildcards.library)),
    log:
        _log("{library}.ataqv.log")
    shell:
        """
        ataqv --peak-file {input.peaks}    \
          --name {params.description}      \
          --metrics-file {output}          \
          --tss-file {params.tss}          \
          --ignore-read-groups             \
          {params.organism}                \
          {input.bam}                      \
          > {log}
        """

##
## Process Samples
##
#:
#: Samples are often run as multiple libraries. We merge libraries and analyze them
#: as single samples. If any libraries are removed from the analysis after QC,
#: the rules below should be be updated to reflect those omissions.
#:

rule make_samples:
    input:
        expand(
            _samples("ataqv", "{sample}.ataqv.json.gz"),
            sample=iterate_all_samples()
        ),
        expand(_samples("gb", "{sample}.bw"), sample=iterate_all_samples()),
        _samples("macs2", "all-samples.master-peaks.bed")


rule merge_libraries:
    input:
        lambda wildcards: expand(
            _libraries("pruned", "{library}.pd.bam"),
            library=iterate_sample_libraries(wildcards.sample)
        )
    output:
        bam = _samples("merge_libraries", "{sample}.bam"),
        bai = _samples("merge_libraries", "{sample}.bam.bai")
    resources: io_concurrent = 1
    threads: 2
    shell:
        """
        samtools merge -@{threads} - {input} \
            | samtools sort -m 4G -O bam -o {output.bam} - 2> /dev/null

        samtools index -@{threads} {output.bam}

        samtools flagstat -@{threads} {output.bam} > {output.bam}.flagstat
        """ 


rule sample_bam2bed:
    input:
        _samples("merge_libraries", "{sample}.bam")
    output:
        _samples("bam2bed", "{sample}.bed")
    shell:
        """bedtools bamtobed -i {input} > {output}"""


rule sample_peaks:
    input:
        _samples("bam2bed", "{sample}.bed")
    output:
        peaks = _samples("macs2", "{sample}_peaks.broadPeak"),
        bdg = _samples("macs2", "{sample}_treat_pileup.bdg.gz"),
    params:
        name = "{sample}",
        outdir = _samples("macs2"),
        genome_size = lambda wildcards: MACS2_GENOME_SIZE[get_sample_genome(wildcards.sample)]
    log:
        _log("{sample}.macs2.out")
    shell:
        """
        macs2 callpeak \
          --outdir {params.outdir} \
          -t {input} \
          -n {params.name} \
          -f BED \
          -g {params.genome_size} \
          --nomodel \
          --shift -100 \
          --extsize 200 \
          --seed 2018 \
          -B \
          --broad \
          --keep-dup all \
          --SPMR \
          &> {log}

        pigz {wildcards.sample}_treat_pileup.bdg
        """


rule sample_noblacklist:
    input:
        _samples("macs2", "{sample}_peaks.broadPeak")
    output:
        _samples("macs2", "{sample}_peaks.broadPeak.noblacklist")
    params:
        blacklists = lambda wildcards: " ".join(
            get_blacklists(get_sample_genome(wildcards.sample))
        )
    shell:
        """mappabilityFilter -i {input} -b {params.blacklists} > {output}"""


rule sample_merged_master_peaks:
    input:
        expand(
            _samples("macs2", "{sample}_peaks.broadPeak.noblacklist"),
            sample=iterate_all_samples()
        )
    output:
        _samples("macs2", "all-samples.master-peaks.bed")
    params:
        fdr = config.get("params_macs2_fdr", 0.05)
    shell:
        """createMasterPeaks {input} --fdr {params.fdr} > {output}"""


rule sample_ataqv:
    """Ataqv-toolkit is a ATAC-seq experiment QC tool developed in Parker Lab.
    The tools provides many useful metrics such as Fragment Length
    Distribution, TSS Enrichment, for quality comparison and downstream
    analysis.

    See: https://github.com/ParkerLab/ataqv
    """
    input:
        bam = _samples("merge_libraries", "{sample}.bam"),
        peaks = _samples("macs2", "{sample}_peaks.broadPeak.noblacklist")
    output:
        _samples("ataqv", "{sample}.ataqv.json.gz")
    params:
        name = "{sample}",
        description = "{sample}",
        organism = lambda wildcards: get_organism(get_sample_genome(wildcards.sample)),
        tss = lambda wildcards: get_tss(get_sample_genome(wildcards.sample))
    log:
        _log("{sample}.ataqv.log")
    shell:
        """
        ataqv --peak-file {input.peaks}    \
          --name {params.description}      \
          --metrics-file {output}          \
          --tss-file {params.tss}          \
          --ignore-read-groups             \
          {params.organism}                \
          {input.bam}                      \
          > {log}
        """


rule sample_bigwig:
    input:
        _samples("macs2", "{sample}_treat_pileup.bdg.gz"),
    output:
        _samples("gb", "{sample}.bw")
    params:
        sizes = lambda wildcards: get_chrom_sizes(get_sample_genome(wildcards.sample))
    shell:
        """bdgTobw {input} {params.sizes} {output}"""


#rule sample_track_lines:


##
## 3. (Optional) Perform downsampling; call `rule downsample` directly
##
rule downsample:
    """ Rule that governs downsampling of the processed BAM files.

    Downsampling is desirable when comparing many samples as sequencing-depth
    can easily factor into observed differences.
    """
    input:
        rules.make_samples.input,
        expand(
            _downsampled("ataqv", "{sample}.ataqv.json.gz"),
            sample=iterate_all_samples()
        ),
        expand(_downsampled("gb", "{sample}.bw"), sample=iterate_all_samples()),
        _downsampled("macs2", "all-samples.master-peaks.bed")


rule downsample_bam:
  input:
        rules.merge_libraries.output
  output:
        bam = _downsampled('merge_libraries', "{sample}.dwnsmpl.bam"),
        bai = _downsampled('merge_libraries', "{sample}.dwnsmpl.bam.bai"),
        flagstat = _downsampled('merge_libraries', "{sample}.dwnsmpl.bam.flagstat")
  threads: 1
  params:
        seed = config["params"].get("seed", 2018),
        depth = config["params"].get("subsample_depth")
  run:
      # if not supplied; no need to downsample, just print error and exit
      if params.depth is None:
          sys.stderr.write("ERROR: Subsampling depth not supplied; Exiting!\n")
          sys.exit(1)
      else:
          # Compute desired fraction of reads to sub-sample using samtools
          # flagstat.
          #
          # NOTE: __TOTAL_READS computes the total number of # "single-reads" in the
          # given input BAM file, and then we compute the fraction to subsample
          # in __FRACTION (requires `bc` tools, POSIX so almost always
          # available). The __FRACTION value already contains a "." (period) so
          # we omit that in samtools call. `scale=3` in `bc -l` limits the number
          # of decimal places to 3.
          #
          # Further, simply symlink files if desired depth is more then available
          # number of reads in the file.
          #
          # NOTE: (( . )) is not a typo. Using BASH"s arithmetic context.
        shell(
        """
            __TOTAL_READS=$(samtools view -cF 0x100 {input.bam}) \
            && if (( $_TOTAL_READS < {params.depth} )); then
                ln -s {input.bam} {output.bam}
                ln -s {input.bai} {output.bai}
            else
                __FRACTION=$(bc -l <<< "scale=3; {params.depth}/$__TOTAL_READS") \
                && samtools view -@ {threads} -b -h -s {params.seed}$__FRACTION {input.bam} -o {output.bam}
                samtools index {output.bam}
                samtools flagstat {output.bam} > {output.flagstat}
            fi
        """
        )


rule downsample_call_peaks:
    input:
        _downsampled("merge_libraries", "{sample}.dwnsmpl.bam"),
    output:
        peaks = _downsampled("macs2", "{sample}_peaks.broadPeak"),
        bdg = _downsampled("macs2", "{sample}_treat_pileup.bdg.gz"),
    params:
        name = "{sample}",
        outdir = _samples("macs2"),
        genome_size = lambda wildcards: MACS2_GENOME_SIZE[get_sample_genome(wildcards.sample)]
    log:
        _log("{sample}.dwnsmpl.macs2.out")
    shell:
        """
        macs2 callpeak \
          --outdir {params.outdir} \
          -t {input} \
          -n {params.name} \
          -f BED \
          -g {params.genome_size} \
          --nomodel \
          --shift -100 \
          --extsize 200 \
          --seed 2018 \
          -B \
          --broad \
          --keep-dup all \
          --SPMR \
          &> {log}

        pigz {wildcards.sample}_treat_pileup.bdg
        """


rule downsample_noblacklist:
    input:
        _downsampled("macs2", "{sample}_peaks.broadPeak")
    output:
        _downsampled("macs2", "{sample}_peaks.broadPeak.noblacklist")
    params:
        blacklists = lambda wildcards: " ".join(
            get_blacklists(get_sample_genome(wildcards.sample))
        )
    shell:
        """mappabilityFilter -i {input} -b {params.blacklists} > {output}"""


rule downsample_master_peaks:
    input:
        lambda wildcards: [
            _downsampled("macs2", f"{sample}_peaks.broadPeak.noblacklist")
            for sample in iterate_all_samples()
        ]
    output:
        _downsampled("macs2", "all-samples.master-peaks.bed")
    params:
        fdr = config["params"].get("macs2_fdr", 0.05)
    shell:
        """createMasterPeaks {input} --fdr {params.fdr} > {output}"""


rule downsample_ataqv:
    """Ataqv-toolkit is a ATAC-seq experiment QC tool developed in Parker Lab.
    The tools provides many useful metrics such as Fragment Length
    Distribution, TSS Enrichment, for quality comparison and downstream
    analysis.

    See: https://github.com/ParkerLab/ataqv
    """
    input:
        bam = _downsampled("merge_libraries", "{sample}.dwnsmpl.bam"),
        peaks = _downsampled("macs2", "{sample}_peaks.broadPeak.noblacklist")
    output:
        _downsampled("ataqv", "{sample}.ataqv.json.gz")
    params:
        name = "{sample}",
        description = "{sample}",
        organism = lambda wildcards: get_organism(get_sample_genome(wildcards.sample)),
        tss = lambda wildcards: get_tss(get_sample_genome(wildcards.sample))
    log:
        _log("{sample}.dwnsmpl.ataqv.log")
    shell:
        """
        ataqv --peak-file {input.peaks}    \
          --name {params.description}      \
          --metrics-file {output}          \
          --tss-file {params.tss}          \
          --ignore-read-groups             \
          {params.organism}                \
          {input.bam}                      \
          > {log}
        """


rule downsample_bigwig:
    input:
        _downsampled("macs2", "{sample}_treat_pileup.bdg.gz"),
    output:
        _downsampled("gb", "{sample}.bw")
    params:
        sizes = lambda wildcards: get_chrom_sizes(get_sample_genome(wildcards.sample))
    shell:
        """bdgTobw {input} {params.sizes} {output}"""


##
## debug info
##

rule versions:
    output:
        _versions("version.txt")
    run:
        status = check_utils_version(output)
        if not status:
            print("Check $PATH. Aborting.", file=sys.stderr)
            sys.exit(1)

##
## rule all
##

rule all:
    input:
        rules.versions.output,
        rules.make_libraries.input,
        rules.make_samples.input,
        rules.downsample.input

##
## notification
##

onerror:
  print("Error: Snakemake aborted!")
  shell("mail -s 'Snakemake Job Error: See log inside!' {config['email']} < {log}")


onsuccess:
  print("Success: Snakemake completed!")
  shell("mail -s 'Snakemake Job Completed: Have a Beer!' {config['email']} < {log}")

# vim: syntax=python
