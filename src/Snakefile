#! /usr/bin/env python
#
# The Parker Lab
# theparkerlab.org
#
# University of Michigan, Ann Arbor
#

shell.prefix("export PATH=$(PWD)/bin:$PATH;")

import os
import sys
import shutil
import itertools
import subprocess
from functools import partial


if not config.get("results"):
    print("`results` dir not set. Check config. Exiting.")
    sys.exit(1)

if not config.get("email"):
    print("WARN: User email not set. No notifications will be sent!")

if not config.get("params"):
    print("WARN: Parameters are not set, will use defaults where possible")
    config["params"] = {}


##
## Metadata
##
ORGANISMS = {
    "rn4": "rat",
    "rn5": "rat",
    "rn6": "rat",
    "mm9": "mouse",
    "mm10": "mouse",
    "hg19": "human",
    "hg38": "human",
}

AUTOSOMAL_REFERENCES = {
    "hg19": [f"chr{i}" for i in range(1, 23)],
    "hg38": [f"chr{i}" for i in range(1, 23)],
    "mm9": [f"chr{i}" for i in range(1, 20)],
    "mm10": [f"chr{i}" for i in range(1, 20)],
    "rn4": [f"chr{i}" for i in range(1, 21)],
    "rn5": [f"chr{i}" for i in range(1, 21)],
    "rn6": [f"chr{i}" for i in range(1, 21)],
}

MACS2_GENOME_SIZE = {
    "rn4": "mm",
    "rn5": "mm",
    "rn6": "mm",
    "mm9": "mm",
    "mm10": "mm",
    "hg19": "hs",
    "hg38": "hs",
}


##
## Generate paths
##
_results = partial(os.path.join, config["results"])
_libraries = partial(_results, "libraries")
_samples = partial(_results, "samples")
_downsampled = partial(_results, "downsampled")
_log = partial(_results, "logs")
_versions = partial(_results, "versions")


##
## Helper functions
##
def iterate_all_samples():
    return set([library["sample"] for library in config["libraries"].values()])


def iterate_all_libraries():
    return sorted(config["libraries"].keys())


def iterate_library_readgroups(library):
    return sorted(config["libraries"][library]["readgroups"].keys())


def iterate_sample_libraries(sample):
    for k, v in config["libraries"].items():
        if v["sample"] == sample:
            yield k


def library_to_sample(library):
    return config["libraries"][library]["sample"]


def readgroup_to_library(readgroup):
    for library in iterate_all_libraries():
        for library_readgroup in iterate_library_readgroups(library):
            if readgroup == library_readgroup:
                return library


def iterate_all_readgroups():
    for library in iterate_all_libraries():
        for readgroup in iterate_library_readgroups(library):
            yield readgroup


def list_readgroup_fastqs(readgroup):
    library = readgroup_to_library(readgroup)
    return config["libraries"][library]["readgroups"][readgroup]


def iterate_all_fastqs():
    for readgroup in iterate_all_readgroups():
        for fastq in list_readgroup_fastqs(readgroup):
            yield fastq


def fastq_basename_to_fastq(fastq_basename):
    for fastq in iterate_all_fastqs():
        if fastq_basename == os.path.basename(fastq):
            return fastq
    print(
        f"FATAL: Could not find FASTQ file for {fastq_basename}; Exiting.",
        file=sys.stderr
    )
    sys.exit(1)


def fastq_to_trimmed_fastq(path, fastq):
    clipped_fastq_basename = os.path.basename(fastq).replace(
        ".fastq.gz", ".tr.fastq.gz"
    )
    return os.path.join(path, clipped_fastq_basename)


def get_library_genome(library):
    return config['libraries'][library]['genome']


def get_sample_genome(sample):
    for lib in iterate_all_libraries():
        if config['libraries'][lib]['sample'] == sample:
            return config['libraries'][lib]['genome']


def get_organism(genome):
    return ORGANISMS[genome]


def get_autosomes(genome):
    return AUTOSOMAL_REFERENCES[genome]


def get_bwa_index(genome):
    return config["bwa_index"][genome]


def get_tss(genome):
    return config["tss"][genome]


def get_chrom_sizes(genome):
    return config["chrom_sizes"][genome]


def get_whitelists(genome):
    return config["whitelist"].get(genome)


def get_blacklists(genome):
    return config["blacklist"].get(genome)


def strip_ext(fastq, ext=".fastq.gz"):
    return os.path.basename(fastq).replace(ext, "")


##
## Optional
##
include: "make_samples.smk"
include: "downsample.smk"


##
## Pipeline
##
##   `rule all` is at the end.
##

rule make_libraries:
    input:
        expand(
            _libraries("fastqc", "{fastq}_fastqc.zip"),
            fastq=[strip_ext(x) for x in iterate_all_fastqs()]
        ),
        ## Uncomment if you want FastQC to be run after trimming
        # expand(
        #     _libraries("fastqc_post-clip", "{fastq}.cl_fastqc.zip"),
        #     fastq=[strip_ext(x) for x in iterate_all_fastqs()]
        # ),
        expand(
            _libraries("ataqv", "{library}.ataqv.json.gz"),
            library=iterate_all_libraries()
        )


rule raw_fastqc:
    input:
        lambda wildcards: fastq_basename_to_fastq(
            f"{wildcards.fastq_basename}.fastq.gz"
        )
    output:
        _libraries("fastqc", "{fastq_basename}_fastqc.zip")
    params:
        outdir = _libraries("fastqc")
    log:
        _log("fastqc.{fastq_basename}.log")
    shell:
        """fastqc {input} -o {params.outdir} &> {log}"""


rule clip_reads:
    """Optionally, clip reads for a given list of samples. Useful when reads
    come from two sequencing runs but each with different read lengths."""
    input:
        R1 = lambda wildcards: fastq_basename_to_fastq(
            f"{wildcards.fastq_basename}.R1.fastq.gz"
        ),
        R2 = lambda wildcards: fastq_basename_to_fastq(
            f"{wildcards.fastq_basename}.R2.fastq.gz"
        )
    output:
        R1 = _libraries("clipped", "{fastq_basename}.R1.cl.fastq.gz"),
        R2 = _libraries("clipped", "{fastq_basename}.R2.cl.fastq.gz")
    params:
        libraries = config["params"].get("clip_readgroups"),
        length = config["params"].get("clip_length")
    run:
        # If FASTQ needs clipping, use cutadapt otherwise simply symlink
        #
        if (params.libraries is None) or (params.length is None):
            sys.stderr.write(f"Skipping rule clip_reads. Check params.\n")
            skip = True

        if (wildcards.fastq_basename not in params.libraries) or skip:
            shell(
                "ln -s {input.R1} {output.R1} \
                    && ln -s {input.R2} {output.R2}"
            )
        else:
            shell(
                "cutadapt -u {params.length} -U {params.length} \
                -o {output.R1} -p {output.R2} {input.R1} {input.R2}"
            )


rule trim:
    """Use cta to trim adapter sequences.

    See https://github.com/ParkerLab/cta.
    """
    input:
        R1 = _libraries("clipped", "{fastq_basename}.R1.cl.fastq.gz"),
        R2 = _libraries("clipped", "{fastq_basename}.R2.cl.fastq.gz")
    output:
        R1 = _libraries("trimmed", "{fastq_basename}.R1.tr.fastq.gz"),
        R2 = _libraries("trimmed", "{fastq_basename}.R2.tr.fastq.gz")
    shell:
        """cta {input.R1} {input.R2} {output.R1} {output.R2}"""


rule post_fastqc:
    """Run FastQC after trimming and clipping (if applicable)."""
    input:
        _libraries("trimmed", "{fastq_basename}.tr.fastq.gz"),
    output:
        _libraries("fastqc_post-trim", "{fastq_basename}.tr_fastqc.zip")
    params:
        outdir = _libraries("fastqc_post-trim")
    log:
        _log("fastqc.{fastq_basename}.log")
    shell:
        """
        fastqc {input} -o {params.outdir} &> {log}
        """


rule map:
    """Map reads to reference using BWA MEM algorithm.

    For ATAC-seq data, we use `-I 200,200,5000` flag which correspond to the
    mean, standard deviation, and max limits of the insert size distribution.
    Reads outside this are ignored by BWA. The defaults inferred by BWA are
    not best suited for the typical ATAC-seq insert size distribution if you
    analysis uses larger size fragments.
    """
    input:
        R1 = lambda wildcards: fastq_to_trimmed_fastq(
            _libraries("trimmed"),
            list_readgroup_fastqs(wildcards.readgroup)[0]
        ),
        R2 = lambda wildcards: fastq_to_trimmed_fastq(
            _libraries("trimmed"),
            list_readgroup_fastqs(wildcards.readgroup)[1]
        ),
        index = lambda wildcards: get_bwa_index(get_library_genome(wildcards.library))
    output:
        _libraries("bwa", "{library}______{readgroup}.bam")
    params:
        rg = "\\t".join(["@RG", "ID:{readgroup}", "LB:{library}"])
    threads: 4
    log:
        bwa = _log("map.bwa.{library}______{readgroup}.log"),
        samtools = _log("map.samtools.{library}______{readgroup}.log")
    shell:
        """
        bwa mem             \
            -M              \
            -R              \
            \"{params.rg}\" \
            -I 200,200,5000 \
            -t {threads}    \
            {input.index}   \
            {input.R1}   \
            {input.R2}  \
            2> {log.bwa}    \
        | samtools sort -m 2G -@ {threads} -O bam -o {output} - 2> {log.samtools}
        """


rule merge_readgroups:
    """Readgroups for a library are merged together into a single BAM file."""
    input:
        lambda wildcards: [
            _libraries("bwa", f"{wildcards.library}______{readgroup}.bam")
            for readgroup in iterate_library_readgroups(wildcards.library)
        ]
    output:
        _libraries("merge_readgroups", "{library}.bam")
    resources: io_concurrent = 1
    threads: 1
    run:
        # Don't bother merging if there's only one readgroup per library
        if len(input) > 1:
            shell("samtools merge -@{threads} {output} {input}")
        else:
            shell("ln -s {input} {output}")


rule mark_duplicates:
    """Use picard tools to mark duplicates for each library. Readgroups are
    merged in previous step as Picard can honor Readgroup tag and remove
    duplicates in a read-group aware manner."""
    input:
        bam = _libraries("merge_readgroups", "{library}.bam")
    output:
        bam = _libraries("mark_duplicates", "{library}.md.bam"),
        bai = _libraries("mark_duplicates", "{library}.md.bam.bai")
    params:
        metrics = _libraries("mark_duplicates", "{library}.md.metrics"),
        tmp_dir = config["params"].get("tmp_dir")
    resources: io_concurrent = 2
    log:
        _log("{library}.md.log")
    shell:
        # TODO: Don"t use picard wrapper; use native picard.jar invokation
        """
        picard -m 8G MarkDuplicates       \
            I={input.bam}                 \
            O={output.bam}                \
            ASSUME_SORTED=true            \
            METRICS_FILE={params.metrics} \
            VALIDATION_STRINGENCY=LENIENT \
            TMP_DIR={params.tmp_dir} &> {log}

        samtools index {output.bam}
        """


rule prune:
    """Prune all non-autosomal reads, marked duplicates, and unpaired reads
    giving only properly paired and aligned reads above specified alignment
    quality threshold.
    """
    input:
        bam = _libraries("mark_duplicates", "{library}.md.bam"),
        bai = _libraries("mark_duplicates", "{library}.md.bam.bai")
    output:
        bam = _libraries("pruned", "{library}.pd.bam"),
        bai = _libraries("pruned", "{library}.pd.bam.bai")
    params:
        tmp_dir = _libraries("pruned"),
        mapq = config["params"].get("prune_mapq", 30),
        autosomes = lambda wildcards: get_autosomes(get_library_genome(wildcards.library))
    shell:
        """
        samtools view -b -h -f 3 -F 4 -F 8 -F 256 -F 1024 -F 2048 \
            -q {params.mapq} {input.bam} {params.autosomes} > {output.bam}

        samtools index {output.bam}
        """


rule bam2bed:
    input:
        _libraries("pruned", "{library}.pd.bam")
    output:
        _libraries("bam2bed", "{library}.pd.bed")
    shell:
        """bedtools bamtobed -i {input} > {output}"""


rule peaks:
    input:
        _libraries("bam2bed", "{library}.pd.bed")
    output:
        peaks = _libraries("macs2", "{library}_peaks.broadPeak"),
        bdg = _libraries("macs2", "{library}_treat_pileup.bdg.gz"),
    params:
        name = "{library}",
        outdir = _libraries("macs2"),
        genome_size = lambda wildcards: MACS2_GENOME_SIZE[get_library_genome(wildcards.library)]
    log:
        _log("{library}.macs2.out")
    shell:
        """
        macs2 callpeak \
          --outdir {params.outdir} \
          -t {input} \
          -n {params.name} \
          -f BED \
          -g {params.genome_size} \
          --nomodel \
          --shift -100 \
          --extsize 200 \
          --seed 2018 \
          -B \
          --broad \
          --keep-dup all \
          --SPMR \
          &> {log}

        pigz {params.outdir}/{wildcards.library}_treat_pileup.bdg \
                {params.outdir}/{wildcards.library}_control_lambda.bdg
        """


rule filter_peaks:
    input:
        _libraries("macs2", "{library}_peaks.broadPeak")
    output:
        _libraries("macs2", "{library}_peaks.noblacklist.bed")
    params:
        blacklists = lambda wildcards: " ".join(
            get_blacklists(get_library_genome(wildcards.library))
        ),
        fdr = config["params"].get("macs2_fdr", 0.05)
    shell:
        """mappabilityFilter -i {input} -b {params.blacklists} | \
                createMasterPeaks --fdr {params.fdr} > {output}"""


rule ataqv:
    """Ataqv-toolkit is a ATAC-seq experiment QC tool developed in Parker Lab.
    The tools provides many useful metrics such as Fragment Length
    Distribution, TSS Enrichment, for quality comparison and downstream
    analysis.

    See: https://github.com/ParkerLab/ataqv
    """
    input:
        bam = _libraries("mark_duplicates", "{library}.md.bam"),
        peaks = _libraries("macs2", "{library}_peaks.noblacklist.bed")
    output:
        _libraries("ataqv", "{library}.ataqv.json.gz")
    params:
        name = "{library}",
        description = "{library}",
        organism = lambda wildcards: get_organism(get_library_genome(wildcards.library)),
        tss = lambda wildcards: get_tss(get_library_genome(wildcards.library)),
    log:
        _log("{library}.ataqv.log")
    shell:
        """
        ataqv --peak-file {input.peaks}    \
          --name {params.description}      \
          --metrics-file {output}          \
          --tss-file {params.tss}          \
          --ignore-read-groups             \
          {params.organism}                \
          {input.bam}                      \
          > {log}
        """


##
## rule all
##

include: "check_utils.smk"

rule all:
    input:
        rules.versions.output,
        rules.make_libraries.input,
        rules.make_samples.input,
        rules.downsample.input


##
## notification
##

onerror:
  print("Error: Snakemake aborted!")
  shell("mail -s 'Snakemake Job Error: See log inside!' {config['email']} < {log}")


onsuccess:
  print("Success: Snakemake completed!")
  shell("mail -s 'Snakemake Job Completed: Have a Beer!' {config['email']} < {log}")

# vim: syntax=python
